{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Firts_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bw2SmNljFVPM",
        "outputId": "bc1db5e6-32fa-443f-aefb-b77503bce3bb"
      },
      "source": [
        "pip install tf-agents"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-agents\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/f5/4b5ddf7138d2fdaad2f7d44437372525859183cdac4ffad3fd86a94f8f52/tf_agents-0.8.0-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.12.0)\n",
            "Requirement already satisfied: gym>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.17.3)\n",
            "Collecting tensorflow-probability==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/c0/d6a9212d3e74748474b59e077e85ca577308c808eee93f9d2e11c3f1cc16/tensorflow_probability-0.12.2-py2.py3-none-any.whl (4.8MB)\n",
            "\u001b[K     |████████████████████████████████| 4.8MB 21.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.3.0)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.7/dist-packages (from tf-agents) (3.12.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.0->tf-agents) (1.5.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (0.1.6)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (0.3.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from tensorflow-probability==0.12.2->tf-agents) (4.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.11.3->tf-agents) (56.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.17.0->tf-agents) (0.16.0)\n",
            "Installing collected packages: tensorflow-probability, tf-agents\n",
            "  Found existing installation: tensorflow-probability 0.12.1\n",
            "    Uninstalling tensorflow-probability-0.12.1:\n",
            "      Successfully uninstalled tensorflow-probability-0.12.1\n",
            "Successfully installed tensorflow-probability-0.12.2 tf-agents-0.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYrihmSHNiXM"
      },
      "source": [
        "\n",
        "from collections import deque\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import keras\n",
        "from tf_agents.networks import q_rnn_network\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from tf_agents.environments import py_environment\n",
        "from tf_agents.environments import tf_environment\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.environments import utils\n",
        "from tf_agents.specs import array_spec\n",
        "from tf_agents.environments import wrappers\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "import tensorflow as tf\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.utils import common\n",
        "import os\n",
        "import tempfile\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.networks import q_network\n",
        "from tf_agents.utils import common\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from tf_agents.drivers import dynamic_episode_driver"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAalzpX6OK93"
      },
      "source": [
        "### reading data from csv and saving to dicts\n",
        "\n",
        "\n",
        "days_prices = defaultdict(dict)\n",
        "counter = 0\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/totrain\"\n",
        "for files in os.listdir(path):\n",
        "    counter+=1\n",
        "    np_dict = defaultdict(dict)\n",
        "    prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/totrain/{files}\", index_col = 0)\n",
        "    prices = prices.drop(\"time\", axis=1)\n",
        "    l = len(prices.columns)\n",
        "    for k in range(0,l):\n",
        "        prices = pd.read_csv(f\"/content/drive/MyDrive/Colab Notebooks/totrain/{files}\", index_col = 0)\n",
        "        prices = prices.drop(\"time\", axis=1)\n",
        "        prices = prices.iloc[:, k:k+1]\n",
        "        for i in prices.columns:\n",
        "            prices[f\"mov_5{i}\"] = prices[f\"{i}\"].rolling(window=5).mean()\n",
        "            prices[f\"mov_20{i}\"] = prices[f\"{i}\"].rolling(window=20).mean()\n",
        "            prices[f\"mov_80{i}\"] = prices[f\"{i}\"].rolling(window=80).mean()\n",
        "            prices = prices.dropna()\n",
        "            prices[\"5-20\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_20{i}\"])/prices[f\"{i}\"], 4)\n",
        "            prices[\"5-80\"] = np.round((prices[f\"mov_5{i}\"] - prices[f\"mov_80{i}\"])/prices[f\"{i}\"], 4)\n",
        "            prices[\"20-80\"] = np.round((prices[f\"mov_20{i}\"] - prices[f\"mov_80{i}\"])/prices[f\"{i}\"], 4)\n",
        "            np_dict[f\"np_diff_{i}\"] = np.array([prices[\"5-20\"].values.reshape(-1,1), prices[\"5-80\"].values.reshape(-1,1), prices[\"20-80\"].values.reshape(-1,1), prices[f\"{i}\"].values.reshape(-1,1) ])\n",
        "    days_prices[f\"np_dict_{counter}\"] = np_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ED5_DbOrZ5"
      },
      "source": [
        "### defining max length of training steps\n",
        "\n",
        "\n",
        "def max_steps_lenght(days_prices):\n",
        "    max_steps = 0\n",
        "    for i in days_prices.keys():\n",
        "        crypto_dict = days_prices[i].keys()\n",
        "        number_of_crypto = len(crypto_dict)\n",
        "        first_dict = list(crypto_dict)[0]\n",
        "        lenght = len(days_prices[i][\"np_diff_XLMUSDT\"][0])-60\n",
        "        lenght = lenght * (number_of_crypto) # timestamp\n",
        "        max_steps += lenght\n",
        "    return max_steps\n",
        "max_steps_lenght(days_prices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6eCSqRiTjM"
      },
      "source": [
        "### creating environment\n",
        "\n",
        "\n",
        "class CustomEnv(py_environment.PyEnvironment):\n",
        "\n",
        "    def __init__(self, days_price, initial_balance, lookback_window_size):\n",
        "\n",
        "        self.lookback_window_size = lookback_window_size\n",
        "\n",
        "        self._action_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(), dtype=np.int32, minimum=0, maximum=1, name='action')\n",
        "\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(\n",
        "            shape=(self.lookback_window_size * 3 + 1,), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.days_price = days_price\n",
        "\n",
        "        self.days_count = 1 \n",
        "\n",
        "        self.price_dict = days_price[f\"np_dict_{self.days_count}\"]\n",
        "\n",
        "        self.dict_names = list(self.price_dict.keys())\n",
        "\n",
        "        self.dict_count = 0\n",
        "\n",
        "        self.initial_balance = initial_balance\n",
        "\n",
        "        self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "\n",
        "        self.episode_ended = False\n",
        "\n",
        "        self.active_position = 0\n",
        "\n",
        "        self.action = 0\n",
        "\n",
        "        self.fees = 0.001\n",
        "\n",
        "        self.spread = 0.001\n",
        "\n",
        "        self.money = initial_balance\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.cum_reward = 0\n",
        "\n",
        "        self.crypto_held = 0\n",
        "\n",
        "        self.previous_money = initial_balance\n",
        "\n",
        "        self._state = False\n",
        "        \n",
        "        self.price_bought = 0\n",
        "\n",
        "    def action_spec(self):\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        self.episode_ended = False\n",
        "        self.current_step = 0\n",
        "        self.price_bought = 0\n",
        "        self.money = self.initial_balance\n",
        "        self.active_position = 0\n",
        "        self.action = 0\n",
        "        self.crypto_held = 0\n",
        "        self.dict_count += 1\n",
        "        if self.dict_count == len(self.dict_names):\n",
        "            self.days_count += 1\n",
        "            self.dict_count = 0\n",
        "            self.price_dict = self.days_price[f\"np_dict_{self.days_count}\"]\n",
        "            self.dict_names = list(self.price_dict.keys())\n",
        "            self.last_step = len(self.price_dict[f\"{self.dict_names[self.dict_count]}\"][0]) - self.lookback_window_size\n",
        "        self.mov_price = self.price_dict[f\"{self.dict_names[self.dict_count]}\"]\n",
        "        self.previous_money = self.initial_balance\n",
        "        self.cum_reward = 0  ###Only now\n",
        "        self.pnl = 0\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "\n",
        "        self._state = np.array(\n",
        "            [self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "             self.mov_price[2, self.current_step:self.last]])\n",
        "\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "\n",
        "        return ts.restart(np.array(self._state, dtype=np.float32))\n",
        "\n",
        "    def open_position(self, action, current_price):\n",
        "        reward = 0\n",
        "        if (self.active_position == 0):\n",
        "            if action == 1:\n",
        "                self.active_position = 1\n",
        "                if self.money <= self.initial_balance * 0.1:\n",
        "                    self.episode_ended = True\n",
        "                    reward = -200\n",
        "                else:\n",
        "                    reward = 0\n",
        "                    self.money = self.money - (self.money * self.spread) - (self.money * self.fees)\n",
        "                    self.crypto_held = (self.money / current_price)\n",
        "                    self.price_bought = current_price\n",
        "                    \n",
        "        elif (self.active_position == 1):\n",
        "            if action == 0:\n",
        "                self.active_position = 0\n",
        "                self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "                reward = (current_price - self.price_bought) * self.crypto_held * 2\n",
        "                self.crypto_held = 0\n",
        "                self.price_bought = 0\n",
        "            else:\n",
        "                self.money = self.crypto_held * current_price\n",
        "                reward = 0\n",
        "\n",
        "        return reward  # think about money balance\n",
        "\n",
        "    def next_observation(self):\n",
        "\n",
        "        self.last = self.current_step + self.lookback_window_size\n",
        "        previous_price = self.mov_price[3, self.lookback_window_size + self.current_step - 1]\n",
        "        current_price = self.mov_price[3, self.lookback_window_size + self.current_step]\n",
        "        obs = np.array([self.mov_price[0, self.current_step:self.last], self.mov_price[1, self.current_step:self.last],\n",
        "                        self.mov_price[2, self.current_step:self.last]])\n",
        "        self.current_step += 1\n",
        "        return obs, current_price, previous_price \n",
        "\n",
        "    def _step(self, action):\n",
        "        reward = 0\n",
        "        done = False\n",
        "        #if self.episode_ended == True:\n",
        "        #    return self._reset()\n",
        "        obs, current_price, previous_price = self.next_observation()\n",
        "        self._state = obs\n",
        "        if (action == 1):\n",
        "            self.action = action\n",
        "            reward = self.open_position(action, current_price)\n",
        "\n",
        "        elif (action == 0):\n",
        "            if self.active_position == 0:\n",
        "                reward = -2\n",
        "            else:\n",
        "                self.action = action\n",
        "                reward = self.open_position(action, current_price)\n",
        "\n",
        "        if self.last == self.last_step-1: ##################################################################check should be if last = last.step -1\n",
        "            self.episode_ended = True\n",
        "\n",
        "        if self.episode_ended == True:\n",
        "            done = True\n",
        "            self.money = self.crypto_held * current_price - (self.money * self.spread) - (self.money * self.fees)\n",
        "            self.active_position = 0\n",
        "            reward = self.money - self.initial_balance\n",
        "\n",
        "        pnl = (self.money - self.previous_money)\n",
        "        self.previous_money = self.money\n",
        "        self.pnl += pnl\n",
        "        info = {}\n",
        "        self._state = np.append(self._state, self.active_position)\n",
        "        # return self._state, reward, done, info\n",
        "        if self.episode_ended == True:\n",
        "            return ts.termination(np.array(self._state, dtype=np.float32), float(reward))\n",
        "        else:\n",
        "            return ts.transition(\n",
        "                np.array(self._state, dtype=np.float32), reward=float(reward), discount=0)\n",
        "\n",
        "    def render(self):\n",
        "        print(f'Step: {self.current_step}, Money: {self.money}, PnL{self.pnl}, Cum reward{self.cum_reward}')\n",
        "    #####SKALIBROWAĆ REWARD\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TgV3mmmEqv7"
      },
      "source": [
        "#### creating model and agent\n",
        "\n",
        "\n",
        "class Agent_net():\n",
        "\n",
        "    def __init__(self, env):\n",
        "        # defining hyperparameters\n",
        "        self.env = env\n",
        "\n",
        "        self.train_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        self.eval_env = tf_py_environment.TFPyEnvironment(self.env)\n",
        "        # network configuration\n",
        "        self.input_fc_layer_params = (181,)\n",
        "        self.lstm_size = (60,)\n",
        "        self.output_fc_layer_params = (60,)\n",
        "        self.learning_rate = 0.005\n",
        "        self.optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "        self.td_errors_loss_fn = common.element_wise_squared_loss\n",
        "        self.train_step_counter = tf.compat.v1.train.get_or_create_global_step() #tf.Variable(0)  \n",
        "        self.replay_buffer_max_length = 1000000\n",
        "        self.batch_size = 64\n",
        "        self.epsilon = 0.2\n",
        "\n",
        "    def qrnn(self):\n",
        "        # create a q_RNNnet\n",
        "        self.q_net = q_rnn_network.QRnnNetwork(\n",
        "            self.train_env.observation_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            input_fc_layer_params=self.input_fc_layer_params,\n",
        "            lstm_size=self.lstm_size,\n",
        "            output_fc_layer_params=self.output_fc_layer_params)\n",
        "\n",
        "    def agent_create(self):\n",
        "        self.qrnn()\n",
        "        self.target = self.qrnn()\n",
        "        self.agent = dqn_agent.DqnAgent(\n",
        "            self.train_env.time_step_spec(),\n",
        "            self.train_env.action_spec(),\n",
        "            q_network=self.q_net,\n",
        "            #target_q_network = self.target,\n",
        "            #target_update_tau = 0.1,\n",
        "            #target_update_period = 100,\n",
        "            epsilon_greedy = self.epsilon,\n",
        "            optimizer=self.optimizer,\n",
        "            td_errors_loss_fn=self.td_errors_loss_fn,\n",
        "            train_step_counter=self.train_step_counter)\n",
        "\n",
        "        self.agent.initialize() # just to start agent\n",
        "        self.eval_policy = self.agent.policy # return the policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(self.train_env.time_step_spec(),\n",
        "                                                             self.train_env.action_spec())\n",
        "        return (self.agent, self.train_env, self.eval_env)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEbTvZ-Md7K3"
      },
      "source": [
        "def r_buffer(agent, train_env, replay_buffer_max_length):\n",
        "    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "        data_spec=agent.collect_data_spec, # getting data scpecification \n",
        "        batch_size=train_env.batch_size, # how many experience there are in a batch\n",
        "        max_length=replay_buffer_max_length) # maximum capacity of buffer is max_lenght * batch size (stored in a tuple)\n",
        "    return replay_buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UpNQPW4ST6T"
      },
      "source": [
        "def driver_collect(replay_buffer, train_env, agent):\n",
        "  #replay_buffer.clear()\n",
        "  observers = [replay_buffer.add_batch]       \n",
        "  driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
        "            train_env, agent.policy, observers, num_episodes=1)\n",
        "  driver.run()\n",
        "  return replay_buffer\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGBKP_Eld9xk"
      },
      "source": [
        "def iter_data(replay_buffer, train_env):\n",
        "  AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "  ns = replay_buffer.num_frames()\n",
        "  ns = ns.numpy()\n",
        "  dataset = replay_buffer.as_dataset(\n",
        "      num_parallel_calls=AUTOTUNE, #how many process at time, if none sequential\n",
        "      sample_batch_size=train_env.batch_size, #how many batches to get\n",
        "      num_steps=ns).prefetch(AUTOTUNE) #how many steps in a batch  \n",
        "  iterator = iter(dataset)\n",
        "  return (iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJjWDh30O58x"
      },
      "source": [
        "### computing reward returned in order to work collect_step() must be first\n",
        "\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_eval_episodes):\n",
        "\n",
        "    total_return = 0.0\n",
        "    for _ in range(num_eval_episodes): # number o evaluation episodes\n",
        "\n",
        "        time_step = environment.reset() # getting first time_step\n",
        "        episode_return = 0\n",
        "        policy_state = policy.get_initial_state(batch_size=env.batch_size)\n",
        "        avg_return = 0\n",
        "        \n",
        "\n",
        "        while not time_step.is_last():\n",
        "            action_step = policy.action(time_step, policy_state) # action taken for given timestep\n",
        "            time_step = environment.step(action_step.action) # assign next time_step \n",
        "            episode_return += time_step.reward # reward taken from next time_step ?\n",
        "           \n",
        "        total_return+= episode_return\n",
        "        print(episode_return)\n",
        "    avg_return = total_return  / num_eval_episodes\n",
        "    return avg_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h48o0ejxRIUS"
      },
      "source": [
        "# agent=None\n",
        "# env = None\n",
        "# tf.compat.v1.reset_default_graph()\n",
        "# env = CustomEnv(days_prices, 1000, 60)\n",
        "# na = Agent_net(env)\n",
        "# agent, train_env, eval_env = na.agent_create()\n",
        "# replay_buffer = r_buffer(agent, train_env, 500000)\n",
        "# count = 0\n",
        "# for _ in range(119):  #######docelowo 120*len(days_prices.keys())\n",
        "#       count+=1\n",
        "#       replay_buffer = driver_collect(replay_buffer, train_env, agent)\n",
        "#       iterator = iter_data(replay_buffer, train_env)\n",
        "#       print(count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAcomA01E4xc"
      },
      "source": [
        "### training\n",
        "\n",
        "\n",
        "def training(agent, train_env, replay_buffer, eval_env, iterator):\n",
        "    num_iterations = 1000000  \n",
        "    collect_steps_per_iteration = 10 \n",
        "    log_interval = 10\n",
        "    eval_interval = 50\n",
        "    num_eval_episodes = 3\n",
        "    index_out = False\n",
        "    n_episodes = 120 * len(days_prices.keys())\n",
        "    try:\n",
        "        % % time # used for prolonged training\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "    agent.train = common.function(agent.train)\n",
        "\n",
        "    # Evaluate the agent's policy once before training.\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes) #use created function\n",
        "    returns = [avg_return]\n",
        "\n",
        "    for _ in range(119):\n",
        "          try:\n",
        "            replay_buffer = driver_collect(replay_buffer, train_env, agent)\n",
        "            iterator = iter_data(replay_buffer, train_env)\n",
        "            \n",
        "\n",
        "            # Sample a batch of data from the buffer and update the agent's network.\n",
        "            experience, unused_info = iterator.get_next() \n",
        "            train_loss = agent.train(experience).loss\n",
        "          except IndexError:\n",
        "            index_out = True\n",
        "\n",
        "          step = agent.train_step_counter.numpy()\n",
        "\n",
        "          if step % log_interval == 0:\n",
        "              print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "              \n",
        "          if step % eval_interval == 0:\n",
        "              try:\n",
        "                  avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "                  print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "                  returns.append(avg_return)\n",
        "              except Exception:\n",
        "                  print(\"index error\")\n",
        "                  index_out = True\n",
        "          if index_out == True:\n",
        "            break\n",
        "\n",
        "    return agent.train_step_counter, agent, replay_buffer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OJx_i18PB2W"
      },
      "source": [
        "### calling env, agent creation, training etc\n",
        "agent=None\n",
        "env = None\n",
        "tf.compat.v1.reset_default_graph()\n",
        "env = CustomEnv(days_prices, 1000, 60)\n",
        "na = Agent_net(env)\n",
        "agent, train_env, eval_env = na.agent_create()\n",
        "replay_buffer = r_buffer(agent, train_env, 50000)\n",
        "save_train_step_counter, save_agent, save_replay_buffer = training(agent, train_env, replay_buffer, eval_env, iterator)\n",
        "\n",
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    model = na.q_net,\n",
        "    agent=save_agent,\n",
        "    policy=save_agent.policy,\n",
        "    replay_buffer=save_replay_buffer,\n",
        "    global_step=save_agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(save_agent.policy)\n",
        "checkpoint = train_checkpointer.save(save_agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkVPodEJWDyR"
      },
      "source": [
        "chkpdir = \"/content/drive/MyDrive/Colab Notebooks/policies\"\n",
        "policy_dir = os.path.join(chkpdir, 'policy')\n",
        "checkpoint_dir = os.path.join(chkpdir, 'checkpoint')\n",
        "train_checkpointer = common.Checkpointer(\n",
        "    ckpt_dir=checkpoint_dir,\n",
        "    max_to_keep=1,\n",
        "    agent=agent,\n",
        "    policy=agent.policy,\n",
        "    replay_buffer=replay_buffer,\n",
        "    global_step=agent.train_step_counter\n",
        ")\n",
        "\n",
        "tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "checkpoint = train_checkpointer.save(agent.train_step_counter)\n",
        "#train_checkpointer.initialize_or_restore()\n",
        "#global_step = tf.compat.v1.train.get_global_step()\n",
        "tf_policy_saver.save(policy_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_Kf0ADu2N31"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}